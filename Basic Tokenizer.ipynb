{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9ea713-f59c-4ce6-ba9e-368b1ebcaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        self.merge={}\n",
    "        self.vocab_size=None\n",
    "        \n",
    "    def getstat(self,tokens):\n",
    "        count={}\n",
    "        for p0,p1 in zip(tokens[:],tokens[1:]):\n",
    "            count[(p0,p1)]=count.get((p0,p1),0)+1\n",
    "        return count \n",
    "    def mergepair(self,ids,pair,idx):\n",
    "        new_tokens=[]\n",
    "        i=0\n",
    "        while i<len(ids):\n",
    "            if i<(len(ids)-1) and ids[i]==pair[0] and ids[i+1]==pair[1]:\n",
    "                new_tokens.append(idx)\n",
    "                i+=2\n",
    "            else:\n",
    "                new_tokens.append(ids[i])\n",
    "                i+=1\n",
    "        return new_tokens\n",
    "    def train(self,text,vocab_size,verbose=False):\n",
    "        #read the data using utf-8 encoding\n",
    "        if vocab_size>=256:\n",
    "            self.vocab_size=vocab_size\n",
    "            tokens=list(text.encode(\"utf-8\"))\n",
    "            tokens=list(map(int,tokens)) \n",
    "            for i in range(vocab_size-256):\n",
    "                getstats=self.getstat(tokens)\n",
    "                maxpair=max(getstats,key=getstats.get)\n",
    "                if len(maxpair):\n",
    "                    tokens=self.mergepair(tokens,maxpair,256+i)\n",
    "                    self.merge[maxpair]=256+i\n",
    "                    if verbose:\n",
    "                        print(f\"Pair {maxpair} merged into {256+i}\")\n",
    "            vocab={k:bytes([k]) for k in range(256)}\n",
    "            for k,v in self.merge.items():\n",
    "                vocab[v]=vocab[k[0]]+vocab[k[1]]\n",
    "            self.vocab=vocab\n",
    "        else:\n",
    "            print(\"The vocab size should be greater than or equal to 256\")\n",
    "        \n",
    "    def encode(self,text):\n",
    "        tokens=list(text.encode('utf-8'))\n",
    "        while len(tokens)>=2:\n",
    "            getstats=self.getstat(tokens)\n",
    "            minpair=min(getstats,key=lambda x:self.merge.get(x,float('inf'))) #gets the pair in merge which has the smallest index\n",
    "            if minpair not in self.merge:\n",
    "                break\n",
    "            tokens=self.mergepair(tokens,minpair,self.merge[minpair])\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "    def decode(self,ids):\n",
    "        tokens_decoded=b\"\".join(self.vocab[x] for x in ids)\n",
    "        text=tokens_decoded.decode('utf-8',errors='replace')\n",
    "        return text\n",
    "\n",
    "    def show_merge_values(self):\n",
    "        for k,v in self.merge.items():\n",
    "            print(f'{k}:{self.vocab[v].decode(\"utf-8\",errors=\"replace\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "053917af-3fad-4117-bec7-5f2305c4a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('taylorswift_wikipedia.txt','r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed81c8e8-f196-40fb-a3e1-ec943c976dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair (101, 32) merged into 256\n",
      "Pair (44, 32) merged into 257\n",
      "Pair (100, 32) merged into 258\n",
      "Pair (46, 32) merged into 259\n",
      "Pair (114, 32) merged into 260\n",
      "Pair (50, 48) merged into 261\n",
      "Pair (115, 32) merged into 262\n",
      "Pair (105, 110) merged into 263\n",
      "Pair (111, 110) merged into 264\n",
      "Pair (114, 105) merged into 265\n",
      "Pair (116, 32) merged into 266\n",
      "Pair (116, 104) merged into 267\n",
      "Pair (101, 258) merged into 268\n",
      "Pair (257, 261) merged into 269\n",
      "Pair (97, 110) merged into 270\n",
      "Pair (97, 114) merged into 271\n",
      "Pair (101, 260) merged into 272\n",
      "Pair (121, 32) merged into 273\n",
      "Pair (97, 108) merged into 274\n",
      "Pair (267, 256) merged into 275\n",
      "Pair (118, 268) merged into 276\n",
      "Pair (119, 105) merged into 277\n",
      "Pair (101, 114) merged into 278\n",
      "Pair (264, 32) merged into 279\n",
      "Pair (277, 102) merged into 280\n",
      "Pair (82, 101) merged into 281\n",
      "Pair (83, 280) merged into 282\n",
      "Pair (111, 260) merged into 283\n",
      "Pair (99, 104) merged into 284\n",
      "Pair (269, 49) merged into 285\n",
      "Pair (111, 109) merged into 286\n",
      "Pair (98, 272) merged into 287\n",
      "Pair (32, 275) merged into 288\n",
      "Pair (97, 121) merged into 289\n",
      "Pair (101, 110) merged into 290\n",
      "Pair (111, 114) merged into 291\n",
      "Pair (274, 32) merged into 292\n",
      "Pair (101, 109) merged into 293\n",
      "Pair (46, 10) merged into 294\n",
      "Pair (265, 101) merged into 295\n",
      "Pair (263, 103) merged into 296\n",
      "Pair (269, 50) merged into 297\n",
      "Pair (116, 105) merged into 298\n",
      "Pair (289, 108) merged into 299\n"
     ]
    }
   ],
   "source": [
    "tokenizer=BasicTokenizer()\n",
    "tokenizer.train(text,300,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e582dc9b-2c8f-4d92-94d9-e20a0c477459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 32, 104, 111, 119, 32, 271, 256, 121, 111, 117, 63]\n"
     ]
    }
   ],
   "source": [
    "encoded_text=tokenizer.encode(\"Hello how are you?\")\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c4922b-4035-4ab4-b30c-8f1da35ff32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text=tokenizer.decode(encoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e749259-b2f1-4680-9b89-3cb156de242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\n"
     ]
    }
   ],
   "source": [
    "text=\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\"\n",
    "text2=tokenizer.decode(tokenizer.encode(text))\n",
    "print(text==text2)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1a7ffb-f9e4-40b9-bf3f-c054c0fd9b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 32):e \n",
      "(44, 32):, \n",
      "(100, 32):d \n",
      "(46, 32):. \n",
      "(114, 32):r \n",
      "(50, 48):20\n",
      "(115, 32):s \n",
      "(105, 110):in\n",
      "(111, 110):on\n",
      "(114, 105):ri\n",
      "(116, 32):t \n",
      "(116, 104):th\n",
      "(101, 258):ed \n",
      "(257, 261):, 20\n",
      "(97, 110):an\n",
      "(97, 114):ar\n",
      "(101, 260):er \n",
      "(121, 32):y \n",
      "(97, 108):al\n",
      "(267, 256):the \n",
      "(118, 268):ved \n",
      "(119, 105):wi\n",
      "(101, 114):er\n",
      "(264, 32):on \n",
      "(277, 102):wif\n",
      "(82, 101):Re\n",
      "(83, 280):Swif\n",
      "(111, 260):or \n",
      "(99, 104):ch\n",
      "(269, 49):, 201\n",
      "(111, 109):om\n",
      "(98, 272):ber \n",
      "(32, 275): the \n",
      "(97, 121):ay\n",
      "(101, 110):en\n",
      "(111, 114):or\n",
      "(274, 32):al \n",
      "(101, 109):em\n",
      "(46, 10):.\n",
      "\n",
      "(265, 101):rie\n",
      "(263, 103):ing\n",
      "(269, 50):, 202\n",
      "(116, 105):ti\n",
      "(289, 108):ayl\n"
     ]
    }
   ],
   "source": [
    "#Showing what all values were merged\n",
    "tokenizer.show_merge_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
